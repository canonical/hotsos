potential-issues:
  system:
    SystemWarnings:
      - Unattended upgrades are enabled which can lead to uncontrolled changes to
        this environment. If maintenance windows are required please consider disabling
        unattended upgrades. (origin=system.auto_scenario_check)
  storage:
    CephOSDWarnings:
      - Found the osd(s) [0, 1, 2] using bcache and their underlying block device
        of these OSDS appear to be SSDs. It's nearly always the case that the benefit
        of using bcache for SSD OSDs is nil and can even adversely affect performance
        in some cases. So this is likely to be a misconfiguration and it's probably
        better to remove the bcache and use the OSDs directly instead. Please compare
        the IOPs of the SSD (OSDs) vs. the bcache device (SSD/NVMe) to ascertain.
        (origin=storage.auto_scenario_check)
    CephWarnings:
      - Ceph cluster is in 'HEALTH_WARN' state. Please check 'ceph health detail'
        for details. (origin=storage.auto_scenario_check)
bugs-detected:
  storage:
    - desc: This Ceph cluster is vulnerable to bug https://tracker.ceph.com/issues/53729
        in which OSD(s) can consume considerable amounts of memory and eventually
        be OOM killed due to potentially millions of duplicate entries for PG(s).
        If affected, it is very hard to recover the OSD(s) so the recommendation is
        to get the cluster upgraded to a version containing a fix if available. At
        the time of writing a partial fix is available for Octopus/15.2.17 (offline
        PG trimming via ceph-objectstore-tool) but Octopus, Pacific and Quincy are
        still pending release of the full fix. See https://bugs.launchpad.net/ubuntu/+source/ceph/+bug/1978913
        for full update on Ubuntu package fixes. Please disable PG autoscaler for
        all pools (set them to 'warn' mode) as PG splitting/merging can exacerbate
        the possibility of hitting this bug.
      id: https://tracker.ceph.com/issues/53729
      origin: storage.auto_scenario_check
