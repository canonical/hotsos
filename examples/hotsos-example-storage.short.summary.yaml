potential-issues:
  system:
    SystemWarnings:
      - Unattended upgrades are enabled which can lead to uncontrolled changes to
        this environment. If maintenance windows are required please consider disabling
        unattended upgrades.
  storage:
    CephOSDWarnings:
      - Found the osd(s) [0, 1, 2] using bcache and their underlying block device
        of these OSDS appear to be SSDs. It's nearly always the case that the benefit
        of using bcache for SSD OSDs is nil and can even adversely affect performance
        in some cases. So this is likely to be a misconfiguration and it's probably
        better to remove the bcache and use the OSDs directly instead. Please compare
        the IOPs of the SSD (OSDs) vs. the bcache device (SSD/NVMe) to ascertain.
    CephWarnings:
      - Ceph cluster is in 'HEALTH_WARN' state. Please check 'ceph health detail'
        for details.
bugs-detected:
  storage:
    https://tracker.ceph.com/issues/53729: This Ceph cluster is vulnerable to a bug
      in which OSDs can consume considerable amounts of memory and eventually be OOM
      killed due to potentially millions of duplicate PG entries. If affected, it
      is very hard to recover the OSDs so the recommendation is to get the cluster
      upgraded to a version containing a fix. The fixes are available as of 15.2.17-0ubuntu0.20.04.4
      (Octopus), 16.2.11 (Pacific) and 17.2.5 (Quincy). For some earlier versions
      of Octopus it is possible to do offline trimming using ceph-objectstore-tool.
      Please disable PG autoscaler for all pools (set them to 'warn' mode) as PG splitting/merging
      can exacerbate the possibility of hitting this bug.
