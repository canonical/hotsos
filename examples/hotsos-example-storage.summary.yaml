hotsos:
  version: development
  repo-info: 0e374261
system:
  hostname: juju-04f1e3-1-lxd-0
  os: ubuntu focal
  num-cpus: 2
  load: 2.51, 2.43, 1.94
  virtualisation: lxc
  rootfs: /dev/vda2        308585260 24784824 268055824   9% /
  unattended-upgrades: ENABLED
  date: Thu 10 Feb 09:47:16 UTC 2022
  ubuntu-pro:
    status: not-attached
  uptime: 0d:11h:8m
  sysctl-mismatch:
    kernel.pid_max:
      actual: '4194304'
      expected: '2097152'
  potential-issues:
    SystemWarnings:
      - Unattended upgrades are enabled which can lead to uncontrolled changes to
        this environment. If maintenance windows are required please consider disabling
        unattended upgrades.
storage:
  ceph:
    release:
      name: octopus
      days-to-eol: 3000
    services:
      systemd:
        enabled:
          - ceph-crash
          - ceph-mgr
          - ceph-mon
        masked:
          - ceph-create-keys
        disabled:
          - ceph-mds
          - ceph-osd
          - ceph-radosgw
          - ceph-volume
        generated:
          - radosgw
      ps:
        - ceph-crash (1)
        - ceph-mgr (1)
        - ceph-mon (1)
    dpkg:
      - ceph 15.2.14-0ubuntu0.20.04.2
      - ceph-base 15.2.14-0ubuntu0.20.04.2
      - ceph-common 15.2.14-0ubuntu0.20.04.2
      - ceph-mds 15.2.14-0ubuntu0.20.04.2
      - ceph-mgr 15.2.14-0ubuntu0.20.04.2
      - ceph-mgr-modules-core 15.2.14-0ubuntu0.20.04.2
      - ceph-mon 15.2.14-0ubuntu0.20.04.2
      - ceph-osd 15.2.14-0ubuntu0.20.04.2
      - python3-ceph-argparse 15.2.14-0ubuntu0.20.04.2
      - python3-ceph-common 15.2.14-0ubuntu0.20.04.2
      - python3-cephfs 15.2.14-0ubuntu0.20.04.2
      - python3-rados 15.2.14-0ubuntu0.20.04.2
      - python3-rbd 15.2.14-0ubuntu0.20.04.2
      - radosgw 15.2.14-0ubuntu0.20.04.2
    status: HEALTH_WARN
    network:
      cluster:
        eth0@if17:
          addresses:
            - 10.0.0.123
          hwaddr: 00:16:3e:ae:9e:44
          state: UP
          speed: 10000Mb/s
      public:
        eth0@if17:
          addresses:
            - 10.0.0.123
          hwaddr: 00:16:3e:ae:9e:44
          state: UP
          speed: 10000Mb/s
    versions:
      mon:
        - 15.2.14
      mgr:
        - 15.2.14
      osd:
        - 15.2.14
    mgr-modules:
      - balancer
      - crash
      - devicehealth
      - orchestrator
      - pg_autoscaler
      - progress
      - rbd_support
      - status
      - telemetry
      - volumes
      - iostat
      - restful
    crush-rules:
      replicated_rule:
        id: 0
        type: replicated
        pools:
          - device_health_metrics (1)
          - glance (2)
          - cinder-ceph (3)
          - nova (4)
  bugs-detected:
    https://tracker.ceph.com/issues/53729: This Ceph cluster is vulnerable to a bug
      in which OSDs can consume considerable amounts of memory and eventually be OOM
      killed due to potentially millions of duplicate PG entries. If affected, it
      is very hard to recover the OSDs so the recommendation is to get the cluster
      upgraded to a version containing a fix. The fixes are available as of 15.2.17-0ubuntu0.20.04.4
      (Octopus), 16.2.11 (Pacific) and 17.2.5 (Quincy). For some earlier versions
      of Octopus it is possible to do offline trimming using ceph-objectstore-tool.
      Please disable PG autoscaler for all pools (set them to 'warn' mode) as PG splitting/merging
      can exacerbate the possibility of hitting this bug.
  potential-issues:
    CephOSDWarnings:
      - Found the osd(s) [0, 1, 2] using bcache and their underlying block device
        of these OSDS appear to be SSDs. It's nearly always the case that the benefit
        of using bcache for SSD OSDs is nil and can even adversely affect performance
        in some cases. So this is likely to be a misconfiguration and it's probably
        better to remove the bcache and use the OSDs directly instead. Please compare
        the IOPs of the SSD (OSDs) vs. the bcache device (SSD/NVMe) to ascertain.
    CephWarnings:
      - Ceph cluster is in 'HEALTH_WARN' state. Please check 'ceph health detail'
        for details.
