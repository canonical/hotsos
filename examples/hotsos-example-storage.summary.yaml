hotsos:
  version: development
  repo-info: 0e374261
system:
  hostname: juju-04f1e3-1-lxd-0
  os: ubuntu focal
  num-cpus: 2
  load: 2.51, 2.43, 1.94
  virtualisation: lxc
  rootfs: /dev/vda2        308585260 24784824 268055824   9% /
  unattended-upgrades: ENABLED
  date: Thu 10 Feb 09:47:16 UTC 2022
  ubuntu-pro:
    status: not-attached
  uptime: 0d:11h:8m
  sysctl-mismatch:
    kernel.pid_max:
      actual: '4194304'
      expected: '2097152'
  potential-issues:
    SystemWarnings:
      - Unattended upgrades are enabled which can lead to uncontrolled changes to
        this environment. If maintenance windows are required please consider disabling
        unattended upgrades. (origin=system.auto_scenario_check)
storage:
  ceph:
    release:
      name: octopus
      days-to-eol: 3000
    services:
      systemd:
        enabled:
          - ceph-crash
          - ceph-mgr
          - ceph-mon
        masked:
          - ceph-create-keys
        disabled:
          - ceph-mds
          - ceph-osd
          - ceph-radosgw
          - ceph-volume
        generated:
          - radosgw
      ps:
        - ceph-crash (1)
        - ceph-mgr (1)
        - ceph-mon (1)
    dpkg:
      - ceph 15.2.14-0ubuntu0.20.04.2
      - ceph-base 15.2.14-0ubuntu0.20.04.2
      - ceph-common 15.2.14-0ubuntu0.20.04.2
      - ceph-mds 15.2.14-0ubuntu0.20.04.2
      - ceph-mgr 15.2.14-0ubuntu0.20.04.2
      - ceph-mgr-modules-core 15.2.14-0ubuntu0.20.04.2
      - ceph-mon 15.2.14-0ubuntu0.20.04.2
      - ceph-osd 15.2.14-0ubuntu0.20.04.2
      - python3-ceph-argparse 15.2.14-0ubuntu0.20.04.2
      - python3-ceph-common 15.2.14-0ubuntu0.20.04.2
      - python3-cephfs 15.2.14-0ubuntu0.20.04.2
      - python3-rados 15.2.14-0ubuntu0.20.04.2
      - python3-rbd 15.2.14-0ubuntu0.20.04.2
      - radosgw 15.2.14-0ubuntu0.20.04.2
    status: HEALTH_WARN
    network:
      cluster:
        eth0@if17:
          addresses:
            - 10.0.0.123
          hwaddr: 00:16:3e:ae:9e:44
          state: UP
          speed: 10000Mb/s
      public:
        eth0@if17:
          addresses:
            - 10.0.0.123
          hwaddr: 00:16:3e:ae:9e:44
          state: UP
          speed: 10000Mb/s
    versions:
      mon:
        - 15.2.14
      mgr:
        - 15.2.14
      osd:
        - 15.2.14
    mgr-modules:
      - balancer
      - crash
      - devicehealth
      - orchestrator
      - pg_autoscaler
      - progress
      - rbd_support
      - status
      - telemetry
      - volumes
      - iostat
      - restful
    crush-rules:
      replicated_rule:
        id: 0
        type: replicated
        pools:
          - device_health_metrics (1)
          - glance (2)
          - cinder-ceph (3)
          - nova (4)
  bugs-detected:
    - desc: This Ceph cluster is vulnerable to bug https://tracker.ceph.com/issues/53729
        in which OSD(s) can consume considerable amounts of memory and eventually
        be OOM killed due to potentially millions of duplicate entries for PG(s).
        If affected, it is very hard to recover the OSD(s) so the recommendation is
        to get the cluster upgraded to a version containing a fix if available. At
        the time of writing a partial fix is available for Octopus/15.2.17 (offline
        PG trimming via ceph-objectstore-tool) but Octopus, Pacific and Quincy are
        still pending release of the full fix. See https://bugs.launchpad.net/ubuntu/+source/ceph/+bug/1978913
        for full update on Ubuntu package fixes. Please disable PG autoscaler for
        all pools (set them to 'warn' mode) as PG splitting/merging can exacerbate
        the possibility of hitting this bug.
      id: https://tracker.ceph.com/issues/53729
      origin: storage.auto_scenario_check
  potential-issues:
    CephOSDWarnings:
      - Found the osd(s) [0, 1, 2] using bcache and their underlying block device
        of these OSDS appear to be SSDs. It's nearly always the case that the benefit
        of using bcache for SSD OSDs is nil and can even adversely affect performance
        in some cases. So this is likely to be a misconfiguration and it's probably
        better to remove the bcache and use the OSDs directly instead. Please compare
        the IOPs of the SSD (OSDs) vs. the bcache device (SSD/NVMe) to ascertain.
        (origin=storage.auto_scenario_check)
    CephWarnings:
      - Ceph cluster is in 'HEALTH_WARN' state. Please check 'ceph health detail'
        for details. (origin=storage.auto_scenario_check)
