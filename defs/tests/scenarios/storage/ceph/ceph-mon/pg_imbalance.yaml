data-root:
  files:
    sos_commands/ceph_mon/json_output/ceph_osd_df_tree_--format_json-pretty: |
      {"nodes": [{"id": 0, "pgs": 295, "name": "osd.0"},
                 {"id": 1, "pgs": 501, "name": "osd.1"},
                 {"id": 2, "pgs": 200, "name": "osd.2"}]}
  copy-from-original:
    - sos_commands/date/date
    - sos_commands/systemd/systemctl_list-units
    - sos_commands/systemd/systemctl_list-unit-files
mock:
  patch:
    hotsos.core.plugins.storage.ceph.CephCrushMap.ceph_report:
      kwargs:
        new:
          osdmap:
            pools:
              - pool: 1
              - pg_autoscale_mode: 'off'
              - pool: 2
              - pg_autoscale_mode: 'on'
    hotsos.core.plugins.storage.ceph.CephCluster.cluster_has_non_empty_pools:
      kwargs:
        new: true
raised-issues:
  CephCrushError: >-
    Found some Ceph osd(s) with > 500 pgs - this is close to the
    hard limit at which point they will stop creating pgs and
    fail - please investigate.
  CephCrushWarning: >-
    Found some Ceph osd(s) whose pg count is > 30% outside the
    optimal range of 50-200 pgs. This could indicate poor data
    distribution across the cluster and result in
    performance degradation.
