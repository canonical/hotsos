import json
import re

from core.checks import DPKGVersionCompare
from core.issues import (
    issue_types,
    issue_utils,
)
from core.utils import sorted_dict
from core.plugins.storage import (
    bcache,
    ceph,
)
from core.plugins.kernel import KernelChecksBase

YAML_PRIORITY = 1
LP1936136_BCACHE_CACHE_LIMIT = 70
OSD_PG_MAX_LIMIT = 500
OSD_PG_OPTIMAL_NUM = 200


class CephOSDChecks(ceph.CephChecksBase):

    def __init__(self):
        super().__init__()
        self.ceph_osd_df_tree = self.cli.ceph_osd_df_tree()
        self.ceph_versions = self.cli.ceph_versions()

    def check_require_osd_release(self):
        cluster = ceph.CephCluster()
        expected_rname = cluster.daemon_dump('osd').get('require_osd_release')
        if not expected_rname:
            return

        for rname in cluster.daemon_release_names('osd'):
            if expected_rname != rname:
                msg = ("require_osd_release is {} but one or more osds is on "
                       "release {} - needs fixing".format(expected_rname,
                                                          rname))
                issue_utils.add_issue(issue_types.CephOSDError(msg))

    def check_osd_msgr_protocol_versions(self):
        """Check if any OSDs are not using the messenger v2 protocol

        The msgr v2 is the second major revision of Cephâ€™s on-wire protocol
        and should be the default Nautilus onward."""
        v1_osds = []
        cluster = ceph.CephCluster()
        osd_count = int(cluster.daemon_dump('osd').get('max_osd'))
        counter = 0
        if osd_count < 1:
            return
        while counter < osd_count:
            key = "osd.{}".format(counter)
            version_info = cluster.daemon_dump('osd').get(key)
            if version_info and version_info.find("v2:") == -1:
                v1_osds.append(counter+1)
            counter = counter + 1
        if v1_osds:
            msg = ("{} OSDs do not bind to v2 address".format(len(v1_osds)))
            issue_utils.add_issue(issue_types.CephOSDWarning(msg))

    def get_ceph_pg_imbalance(self):
        """ Validate PG counts on OSDs

        Upstream recommends 50-200 OSDs ideally. Higher than 200 is also valid
        if the OSD disks are of different sizes but that tends to be the
        exception rather than the norm.

        We also check for OSDs with excessive numbers of PGs that can cause
        them to fail.
        """
        if not self.ceph_osd_df_tree:
            return

        # Find index of PGS column in output.
        pgs_idx = None
        header = self.ceph_osd_df_tree[0].split()
        for i, col in enumerate(header):
            if col == "PGS":
                pgs_idx = i + 1
                break

        if not pgs_idx:
            return

        pgs_idx = -(len(header) - pgs_idx)
        suboptimal_pgs = {}
        error_pgs = {}
        for line in self.ceph_osd_df_tree:
            try:
                ret = re.compile(r"\s+(osd\.\d+)\s*$").search(line)
                if ret:
                    osd_id = ret.group(1)
                    pgs = int(line.split()[pgs_idx])
                    if pgs > OSD_PG_MAX_LIMIT:
                        error_pgs[osd_id] = pgs

                    margin = abs(100 - (float(100) / OSD_PG_OPTIMAL_NUM * pgs))
                    # allow 10% margin from optimal OSD_PG_OPTIMAL_NUM value
                    if margin > 10:
                        suboptimal_pgs[osd_id] = pgs
            except IndexError:
                pass

        if error_pgs:
            info = sorted_dict(error_pgs, key=lambda e: e[1], reverse=True)
            self._output['osd-pgs-near-limit'] = info
            msg = ("{} osds found with > {} pgs - this is close to the hard "
                   "limit at which point OSDs will stop creating pgs and fail "
                   "- please investigate".
                   format(len(error_pgs), OSD_PG_MAX_LIMIT))
            issue = issue_types.CephCrushError(msg)
            issue_utils.add_issue(issue)

        if suboptimal_pgs:
            info = sorted_dict(suboptimal_pgs, key=lambda e: e[1],
                               reverse=True)
            self._output['osd-pgs-suboptimal'] = info
            msg = ("{} osds found with > 10% margin from optimal {} pgs.".
                   format(len(suboptimal_pgs), OSD_PG_OPTIMAL_NUM))
            issue = issue_types.CephCrushWarning(msg)
            issue_utils.add_issue(issue)

    def get_ceph_versions_mismatch(self):
        """
        Get versions of all Ceph daemons.
        """
        if not self.ceph_versions:
            return

        try:
            data = json.loads(''.join(self.ceph_versions))
        except ValueError:
            return

        versions_set = set()
        daemon_version_info = {}
        if data:
            for unit, _ in data.items():
                if unit == "overall":
                    continue

                vers = []
                for key, _ in data[unit].items():
                    if re.compile(r"ceph version.+").match(key):
                        d_ver = key.split()[2]
                        if d_ver not in vers:
                            vers.append(d_ver)
                            versions_set.add(d_ver)
                if vers:
                    daemon_version_info[unit] = vers

        if daemon_version_info:
            self._output["versions"] = daemon_version_info
            if len(versions_set) > 1:
                msg = ("ceph daemon versions not aligned")
                issue = issue_types.CephDaemonWarning(msg)
                issue_utils.add_issue(issue)

    def build_buckets_from_crushdump(self, crushdump):
        buckets = {}
        # iterate jp for each bucket
        for bucket in crushdump["buckets"]:
            bid = bucket["id"]
            items = []
            for item in bucket["items"]:
                items.append(item["id"])

            buckets[bid] = {"name": bucket["name"],
                            "type_id": bucket["type_id"],
                            "type_name": bucket["type_name"],
                            "items": items}

        return buckets

    def get_crushmap_mixed_buckets(self):
        """
        Report buckets that have mixed type of items,
        as they will cause crush map unable to compute
        the expected up set
        """
        osd_crush_dump = self.cli.ceph_osd_crush_dump_json_decoded()
        if not osd_crush_dump:
            return

        bad_buckets = []
        buckets = self.build_buckets_from_crushdump(osd_crush_dump)
        # check all bucket
        for bid in buckets:
            items = buckets[bid]["items"]
            type_ids = []
            for item in items:
                if item >= 0:
                    type_ids.append(0)
                else:
                    type_ids.append(buckets[item]["type_id"])

            if not type_ids:
                continue

            # verify if the type_id list contain mixed type id
            if type_ids.count(type_ids[0]) != len(type_ids):
                bad_buckets.append(buckets[bid]["name"])

        if bad_buckets:
            msg = ("mixed crush buckets identified (see --storage for more "
                   "info)")
            issue = issue_types.CephCrushWarning(msg)
            issue_utils.add_issue(issue)
            self._output["mixed_crush_buckets"] = bad_buckets

    def check_bcache_vulnerabilities(self):
        has_bcache = False
        for osd in self.local_osds:
            dev = osd.device
            if self.is_bcache_device(dev):
                has_bcache = True

        if not has_bcache:
            return

        for cset in bcache.BcacheChecksBase().get_sysfs_cachesets():
            if (cset.get("cache_available_percent") >=
                    LP1936136_BCACHE_CACHE_LIMIT):
                return

        # Get version of osd based on package installed. This is prone to
        # inaccuracy since the daemon many not have been restarted after
        # package update.
        current = self.apt_check.get_version('ceph-osd')
        if current <= DPKGVersionCompare("13.0.1"):
            return
        if current >= DPKGVersionCompare("14.2.10") and \
           current <= DPKGVersionCompare("14.2.21"):
            return
        if current >= DPKGVersionCompare("15.2.2") and \
           current <= DPKGVersionCompare("15.2.12"):
            return
        if current == DPKGVersionCompare("16.1.0") or \
           current == DPKGVersionCompare("17.0.0"):
            return

        if KernelChecksBase().version >= "5.4":
            return

        bluefs_buffered_io = self.ceph_config.get('bluefs_buffered_io')
        if bluefs_buffered_io is False:
            return

        # NOTE: we need a way to check that actual osd config

        # then bluefs_buffered_io is True by default
        msg = ("host may be vulnerable to bcache bug 1936136 - please ensure "
               "bluefs_buffered_io is set to False or upgrade to kernel "
               ">= 5.4")
        issue = issue_types.CephCrushWarning(msg)
        issue_utils.add_issue(issue)

    def __call__(self):
        if self.local_osds:
            osds = {}
            for osd in self.local_osds:
                osds.update(osd.to_dict())

            self._output["local-osds"] = sorted_dict(osds)

            self.check_bcache_vulnerabilities()

        self.check_require_osd_release()
        self.check_osd_msgr_protocol_versions()
        self.get_ceph_pg_imbalance()
        self.get_ceph_versions_mismatch()
        self.get_crushmap_mixed_buckets()
