data-root:
  files:
    sos_commands/ceph_mon/json_output/ceph_pg_dump_--format_json-pretty: |
      {"pg_map": {"pg_stats": [{"stat_sum": {"num_large_omap_objects": 1},
                                "last_scrub_stamp": "2021-09-16T21:26:00.00",
                                "last_deep_scrub_stamp": "2021-09-16T21:26:00.00",
                                "pgid": "2.f", "state": "active+clean+laggy"}]}}
  copy-from-original:
    - sos_commands/date/date
    - sos_commands/systemd/systemctl_list-units
    - sos_commands/systemd/systemctl_list-unit-files
raised-issues:
  CephWarning: >-
    Large OMAP objects found in pgs '2.f'. This is usually resolved by
    deep-scrubbing the pgs. Check config options
    'osd_deep_scrub_large_omap_object_key_threshold' and
    'osd_deep_scrub_large_omap_object_value_sum_threshold' to
    find whether the values of these keys are too high.
    See https://portal.support.canonical.com/ua/s/article/What-are-large-OMAP-objects-and-how-to-handle-them
    for more info.
    If the large OMAP objects are reported from a pool used by OpenStack Gnocchi,
    it may need tuning: https://portal.support.canonical.com/ua/s/article/Gnocchi-causing-large-OMAP-objects-in-a-Ceph-cluster
