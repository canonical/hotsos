checks:
  osds_have_unusual_raw_usage:
    property:
      path: hotsos.core.plugins.storage.ceph.CephCluster.osd_raw_usage_higher_than_data
      ops: [[length_hint], [gt, 0]]
conclusions:
  osds-have-unusual-raw-usage:
    decision: osds_have_unusual_raw_usage
    raises:
      type: CephOSDWarning
      message: >-
        Found OSD(s) {bad_osds} with larger raw usage size than data+meta+omap+bluefs
        combined. While a discrepancy is to be expected due to Ceph using space
        not accounted by data+meta+omap+bluefs columns, usage is greater than {limit}%
        and likely indicates high discard ops sent to disk which is often
        the case for workloads with frequent rewrites.

        If these OSDs appear full or misbehave please restart them.

        If the problem persists (i.e. OSD restarts do not help) you should disable
        bdev_async_discard for OSDs. For charmed Ceph, this option is controlled
        via the bdev-enable-discard flag which should be set to 'disable'.
      format-dict:
        bad_osds: '@checks.osds_have_unusual_raw_usage.requires.value_actual:comma_join'
        limit: hotsos.core.plugins.storage.ceph.CephCluster.OSD_DISCREPANCY_ALLOWED
